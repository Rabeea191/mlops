{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9539c762",
   "metadata": {
    "id": "9539c762"
   },
   "outputs": [],
   "source": [
    "import librosa \n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import librosa\n",
    "# import soundfile as sf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import librosa.display\n",
    "# import sox\n",
    "# from surfboard.sound import Waveform\n",
    "\n",
    "# import surfboard\n",
    "from sklearn.metrics import matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a780725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 22:01:39.034967: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-08 22:01:39.613903: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-08 22:01:39.613939: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-08 22:01:41.231335: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 22:01:41.231480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 22:01:41.231497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization,Dropout,Conv1D,Add,MaxPooling1D,Lambda,TimeDistributed,Dense,Activation,GlobalMaxPooling1D\n",
    "def _bn_relu(layer, dropout=0, **params):\n",
    "    '''\n",
    "    This function returns a Batch Normalization Layer with desired activation function\n",
    "    '''\n",
    "    layer = BatchNormalization()(layer) \n",
    "    layer = Activation(params[\"conv_activation\"])(layer)\n",
    "\n",
    "    if dropout > 0:\n",
    "        \n",
    "        layer = Dropout(params[\"conv_dropout\"])(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "def add_conv_weight(\n",
    "        layer,\n",
    "        filter_length,\n",
    "        num_filters,\n",
    "        subsample_length=1,\n",
    "        **params):\n",
    "    '''\n",
    "    This function returns a Convolution Layer with desired params\n",
    "    '''\n",
    "    \n",
    "    layer = Conv1D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=filter_length,\n",
    "        strides=subsample_length,\n",
    "        padding='same',\n",
    "        kernel_initializer=params[\"conv_init\"])(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def add_conv_layers(layer, **params):\n",
    "    '''\n",
    "    This function returns a Conv1D and BN Layer stacked together\n",
    "    '''\n",
    "    for subsample_length in params[\"conv_subsample_lengths\"]:\n",
    "        layer = add_conv_weight(\n",
    "                    layer,\n",
    "                    params[\"conv_filter_length\"],\n",
    "                    params[\"conv_num_filters_start\"],\n",
    "                    subsample_length=subsample_length,\n",
    "                    **params)\n",
    "        layer = _bn_relu(layer, **params)\n",
    "    return layer\n",
    "\n",
    "def resnet_block(\n",
    "        layer,\n",
    "        num_filters,\n",
    "        subsample_length,\n",
    "        block_index,\n",
    "        **params):\n",
    "    '''\n",
    "    This function returns a Resnet Block with desired activation function\n",
    "    '''\n",
    "\n",
    "    def zeropad(x):\n",
    "        '''\n",
    "        This function pads zeros to the input vector by a zero vector of same shape in 3rd Dimension\n",
    "        This is used when convolution filters are doubled every 4th Residual Block to match the dimensions\n",
    "        '''\n",
    "        y = tf.zeros_like(x)\n",
    "        return tf.concat([x, y], axis=2)\n",
    "\n",
    "    def zeropad_output_shape(input_shape):\n",
    "        '''\n",
    "        This function checks the shape of input then doubles the 3rd dimension and returns the shape as tuple\n",
    "        This is used to get dimesion shape for the zeropad function\n",
    "        '''\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) == 3\n",
    "        shape[2] *= 2\n",
    "        return tuple(shape)\n",
    "\n",
    "    # Adding Skip Connections\n",
    "    shortcut = MaxPooling1D(pool_size=subsample_length)(layer)\n",
    "    # At each 4th residual block, double the convolution filters, pad the shortcut so that dimensions match\n",
    "    zero_pad = (block_index % params[\"conv_increase_channels_at\"]) == 0 \\\n",
    "        and block_index > 0\n",
    "    if zero_pad is True:\n",
    "        shortcut = Lambda(zeropad, output_shape=zeropad_output_shape)(shortcut)\n",
    "    for i in range(params[\"conv_num_skip\"]):\n",
    "        if not (block_index == 0 and i == 0):\n",
    "            layer = _bn_relu(\n",
    "                layer,\n",
    "                dropout=params[\"conv_dropout\"] if i > 0 else 0,\n",
    "                **params)\n",
    "        layer = add_conv_weight(\n",
    "            layer,\n",
    "            params[\"conv_filter_length\"],\n",
    "            num_filters,\n",
    "            subsample_length if i == 0 else 1,\n",
    "            **params)\n",
    "    layer = Add()([shortcut, layer])\n",
    "    return layer\n",
    "\n",
    "def get_num_filters_at_index(index, num_start_filters, **params):\n",
    "    '''\n",
    "    This function returns the convolution filters for the specified layer\n",
    "    '''\n",
    "    return 2**int(index / params[\"conv_increase_channels_at\"]) \\\n",
    "        * num_start_filters\n",
    "\n",
    "def add_resnet_layers(layer, **params):\n",
    "    '''\n",
    "    This Function addds the residual blocks that make up the structure\n",
    "    The first and last layers of the network are special-cased due to this pre-activation block structure.\n",
    "    '''\n",
    "    layer = add_conv_weight(\n",
    "        layer,\n",
    "        params[\"conv_filter_length\"],\n",
    "        params[\"conv_num_filters_start\"],\n",
    "        subsample_length=1,\n",
    "        **params)\n",
    "    layer = _bn_relu(layer, **params)\n",
    "    for index, subsample_length in enumerate(params[\"conv_subsample_lengths\"]):\n",
    "        num_filters = get_num_filters_at_index(\n",
    "            index, params[\"conv_num_filters_start\"], **params)\n",
    "        layer = resnet_block(\n",
    "            layer,\n",
    "            num_filters,\n",
    "            subsample_length,\n",
    "            index,\n",
    "            **params)\n",
    "    layer = _bn_relu(layer, **params)\n",
    "    return layer\n",
    "    \n",
    "def add_output_layer(layer, **params):\n",
    "    '''\n",
    "    This Function adds the output layer which is a Dense Layer wrapped in a TimeDistributed Layer.\n",
    "    We use TimeDistributed layer so that the model outputs a prediction for each timestep and the temporal information is retained \n",
    "    Because TimeDistributed applies the same instance of Dense to each of the timestamps, the same set of weights are used at each timestamp. \n",
    "    '''\n",
    "    layer = GlobalMaxPooling1D(name = 'feats')(layer)\n",
    "    layer = Dense(params[\"num_categories\"])(layer)\n",
    "    return Activation('sigmoid')(layer)\n",
    "\n",
    "def add_compile(model, **params):\n",
    "    '''\n",
    "    This functions adds the compiler to the model\n",
    "    We have used Adam Optimizer and Categorical Cross-Entropy Loss'''\n",
    "    \n",
    "    # optimizer = Adam(\n",
    "    #     learning_rate=params[\"learning_rate\"],\n",
    "    #     clipnorm=params.get(\"clipnorm\", 1))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_network(**params):\n",
    "    '''\n",
    "    This function builds the entire network based on given parameters'''\n",
    "    inputs = tf.keras.Input(shape=params['input_shape'],\n",
    "                   dtype='float32',\n",
    "                   name='inputs')\n",
    "\n",
    "    if params.get('is_regular_conv', False):\n",
    "        layer = add_conv_layers(inputs, **params)\n",
    "    else:\n",
    "        layer = add_resnet_layers(inputs, **params)\n",
    "\n",
    "    output = add_output_layer(layer, **params)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[output])\n",
    "    if params.get(\"compile\", True):\n",
    "        model = add_compile(model, **params)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aee2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Parameters\n",
    "params = {\n",
    "\"conv_subsample_lengths\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "\"conv_filter_length\": 16,\n",
    "\"conv_num_filters_start\": 32,\n",
    "\"conv_init\": \"he_normal\",\n",
    "\"conv_activation\": \"relu\",\n",
    "\"conv_dropout\": 0.2,\n",
    "\"conv_num_skip\": 2,\n",
    "\"conv_increase_channels_at\": 4,\n",
    "\n",
    "\"learning_rate\": 0.001,\n",
    "\"input_shape\": [22016, 1],\n",
    "\"num_categories\": 1,\n",
    "\"compile\":False\n",
    "}\n",
    "# Create Model\n",
    "model = build_network(**params)\n",
    "model.load_weights('CNN_Weights/cnn_model3_cough_vs_non_cough_segmented_22016_65k_files_orig_train_included_v2_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bd7c3979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2561"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path =  'D:\\AI4LYF\\Covid\\Latest Work\\cohort_3_segmented\\cohort_3_segmented\\pos_seg'\n",
    "path_pos='/home/bigpenguin/projects/project_covid/gathrd_data/cohort3/cohort_3_segmented_mp3/pos_seg/'\n",
    "pos_files = [os.path.join(path_pos,i)  for i in os.listdir(path_pos) if 'ipynb' not in i ]\n",
    "pos_files = np.array(sorted(pos_files))\n",
    "len(pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "97b4d3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8059"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path =  'D:\\AI4LYF\\Covid\\Latest Work\\cohort_3_segmented\\cohort_3_segmented\\\\neg_seg'\n",
    "# path='/home/bigpenguin/projects/surfboard_works/data_for_cough_denoising/coughs/segmented/'\n",
    "path_neg='/home/bigpenguin/projects/project_covid/gathrd_data/cohort3/cohort_3_segmented_mp3/neg_seg/'\n",
    "neg_files = [os.path.join(path_neg,i)  for i in os.listdir(path_neg) if 'ipynb' not in i ]\n",
    "neg_files = np.array(sorted(neg_files))\n",
    "len(neg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7fdd140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [\n",
    "    *pos_files, \n",
    "    *neg_files\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6e013e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10620"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "603e1256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bigpenguin/projects/project_covid/gathrd_data/cohort3/cohort_3_segmented_mp3/pos_seg/pos-0512-019-cough-m-59-5.mp3\n",
      "22284\n"
     ]
    }
   ],
   "source": [
    "X_s = []\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "for file in pos_files:\n",
    "    y, sr = librosa.load(file,sr=22050)\n",
    "    y = librosa.util.normalize(y)\n",
    "    \n",
    "    if len(y)<=22016:\n",
    "\n",
    "        pad_num = 22016-len(y)\n",
    "        y = np.pad(y, [(0), (pad_num)], mode='constant')\n",
    "      \n",
    "    else:\n",
    "        print(file)\n",
    "        print(len(y))\n",
    "        y = y[0:22016]\n",
    "        \n",
    "    X_s.append(y)\n",
    "    \n",
    "    # if pos_count==800:\n",
    "    #   break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9b294497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_s_neg = []\n",
    "for file in neg_files:\n",
    "    y, sr = librosa.load(file,sr=22050)\n",
    "    y = librosa.util.normalize(y)\n",
    "    if len(y)<=22016:\n",
    "\n",
    "        pad_num = 22016-len(y)\n",
    "        y = np.pad(y, [(0), (pad_num)], mode='constant')\n",
    "      \n",
    "    else:\n",
    "#       print(file)\n",
    "#       print(len(y))\n",
    "        y = y[0:22016]\n",
    "    X_s_neg.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "08cba484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2561, 22016), (8059, 22016))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_s = np.array(X_s)\n",
    "X_s_neg = np.array(X_s_neg)\n",
    "X_s.shape,X_s_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e51d895f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 168s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_pos = []\n",
    "y_pred_proba = model.predict(X_s)\n",
    "y_pred = np.where(y_pred_proba>0.97,1,0)\n",
    "for idx,(i,j) in enumerate(zip(y_pred,y_pred_proba)):\n",
    "    if i[0] ==0:\n",
    "    # print(files[idx],str(i[0]),'('+str(round(1-j[0], 4))+')')\n",
    "        indices_pos.append(idx)\n",
    "    elif i[0]==1:\n",
    "    # print(files[idx],str(i[0]),'('+str(round(j[0], 4))+')')\n",
    "        continue\n",
    "len(indices_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3b974e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_proba=model.predict(X_s_neg[0:50000])\n",
    "# y_pred_proba1=model.predict(X_s_neg[50000:100000])\n",
    "# y_pred_proba2=model.predict(X_s_neg[100000:])\n",
    "\n",
    "# len(y_pred_proba),len(y_pred_proba1),len(y_pred_proba2)\n",
    "# ypp=np.array([*y_pred_proba.reshape(-1),*y_pred_proba1.reshape(-1),*y_pred_proba2.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cd88e793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 525s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2365"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_neg = []\n",
    "\n",
    "y_pred_proba = model.predict(X_s_neg)\n",
    "y_pred = np.where(y_pred_proba>0.97,1,0)\n",
    "for idx,(i,j) in enumerate(zip(y_pred,y_pred_proba)):\n",
    "    if i[0] ==0:\n",
    "#         print(files[idx],str(i[0]),'('+str(round(1-j[0], 4))+')')\n",
    "        indices_neg.append(idx)\n",
    "    elif i[0]==1:\n",
    "#         print(files[idx],str(i[0]),'('+str(round(j[0], 4))+')')\n",
    "        continue\n",
    "len(indices_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "52251325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Parameters\n",
    "params = {\n",
    "\"conv_subsample_lengths\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "\"conv_filter_length\": 16,\n",
    "\"conv_num_filters_start\": 32,\n",
    "\"conv_init\": \"he_normal\",\n",
    "\"conv_activation\": \"relu\",\n",
    "\"conv_dropout\": 0.2,\n",
    "\"conv_num_skip\": 2,\n",
    "\"conv_increase_channels_at\": 4,\n",
    "\n",
    "\"learning_rate\": 0.001,\n",
    "\"input_shape\": [22016, 1],\n",
    "\"num_categories\": 1,\n",
    "\"compile\":False\n",
    "}\n",
    "# Create Model\n",
    "model_1 = build_network(**params)\n",
    "model_1.load_weights('/home/bigpenguin/projects/surfboard_works/CNN_Weights/cnn_model3_cough_vs_non_cough_segmented_22016_orig_train_included_final.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "db833b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 169s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_pos_1 = []\n",
    "y_pred_proba = model_1.predict(X_s)\n",
    "y_pred = np.where(y_pred_proba>0.97,1,0)\n",
    "for idx,(i,j) in enumerate(zip(y_pred,y_pred_proba)):\n",
    "    if i[0] ==0:\n",
    "    # print(files[idx],str(i[0]),'('+str(round(1-j[0], 4))+')')\n",
    "        indices_pos_1.append(idx)\n",
    "    elif i[0]==1:\n",
    "    # print(files[idx],sy_pred_proba = model_1.predict(X_s)tr(i[0]),'('+str(round(j[0], 4))+')')\n",
    "        continue\n",
    "len(indices_pos_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "64814df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_proba_1=model_1.predict(X_s_neg[0:50000])\n",
    "# y_pred_proba1_1=model_1.predict(X_s_neg[50000:100000])\n",
    "# y_pred_proba2_1=model_1.predict(X_s_neg[100000:])\n",
    "\n",
    "# len(y_pred_proba_1),len(y_pred_proba1_1),len(y_pred_proba2_1)\n",
    "# ypp_1=np.array([*y_pred_proba_1.reshape(-1),*y_pred_proba1_1.reshape(-1),*y_pred_proba2_1.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "28d7bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 543s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "929"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_neg_1 = []\n",
    "y_pred_proba = model_1.predict(X_s_neg)\n",
    "y_pred = np.where(y_pred_proba>0.97,1,0)\n",
    "\n",
    "for idx,(i,j) in enumerate(zip(y_pred,y_pred_proba)):\n",
    "    if i[0] ==0:\n",
    "        # print(files[idx],str(i[0]),'('+str(round(1-j[0], 4))+')')\n",
    "        indices_neg_1.append(idx)\n",
    "    elif i[0]==1:\n",
    "        # print(files[idx],str(i[0]),'('+str(round(j[0], 4))+')')\n",
    "        continue\n",
    "len(indices_neg_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a5f1bb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_del = set(pos_files[indices_pos]).union( set(pos_files[indices_pos_1]))\n",
    "len(set(pos_files[indices_pos]).union( set(pos_files[indices_pos_1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3a813a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2457"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_del = set(neg_files[indices_neg_1]).union( set(neg_files[indices_neg]))\n",
    "len(set(neg_files[indices_neg_1]).union( set(neg_files[indices_neg])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "91d2a542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2457, 732)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_del),len(pos_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dae94f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!!!!!!!!!!! USE CAREFULLY !!!!!!!!!!!\n",
    "'''\n",
    "\n",
    "# # negative and positive non cough samples deletion from target dirs -- \n",
    "# # p='/home/bigpenguin/projects/project_covid/gathrd_data/COHORT1/cohort_1_segmented_mp3/pos_seg/'\n",
    "# # n='/home/bigpenguin/projects/project_covid/gathrd_data/COHORT1/cohort_1_segmented_mp3/neg_seg/'\n",
    "\n",
    "# def delete_non_cough(target_dir,list_of_files_to_delete):\n",
    "#     [os.remove(target_dir+i) for i in [x.split('/')[-1] for x in list_of_files_to_delete] if i in os.listdir(target_dir)]\n",
    "# #         if i in os.listdir(target_dir):\n",
    "# #             os.remove(target_dir+i)\n",
    "\n",
    "# delete_non_cough(path_pos,pos_del)\n",
    "# delete_non_cough(path_neg,neg_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8f904328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"pos_to_be deleted.npy\",np.array(list(pos_del)))\n",
    "# np.save(\"neg_to_be deleted.npy\",np.array(list(neg_del)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4jwFqloJgJh6",
    "mCmjClpNmGjn",
    "fqhLrvNehw-p",
    "ocPr3XurieCl",
    "WVUiMN5rSc7c",
    "mjjQpfVNw2b3",
    "mzJs2pjbt48_",
    "cgJ5iwvmpVAr",
    "25JmhmKapkB1",
    "GyWjrsiMpkcs"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
